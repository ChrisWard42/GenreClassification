======================================================
=== Genre Classification Methods for English Books ===
===         Nikolai Vogler, Chris Ward             ===
======================================================

Project Overview
----------------
The project is broken down into five discrete python scripts, each performing
a different part of the process and designed to be run independently. Within
each script there are some functions designed as part of a pipeline and others
designed to run independently. In general, the main function in each file will
contain commented out lines intended to run individual parts of the program.

Script files (alphabetical):
    db_operations.py
        Module to perform operations on the manybooks sqlite databases
    learn.py
        Module to execute machine learning algorithms and generate statistics on book data
    process_gutenberg.py
        Module to convert all raw gutenberg files into a usable format with sufficient data
    process_manybooks.py
        Module to download genre classified books from manybooks.net
    statistics.py
        Module to generate statistics for all text files in a directory

Required Python Modules
-----------------------
Chardet - https://pypi.python.org/pypi/chardet
    Used for encoding detection in process_manybooks and db_operation modules

Gutenberg - https://pypi.python.org/pypi/Gutenberg/0.4.2
    Used for stripping file headers in process_gutenberg and process_manybooks modules

Matplotlib - https://pypi.python.org/pypi/matplotlib/1.5.1
    Used for plotting in statistics module

Numpy - https://pypi.python.org/pypi/numpy/1.10.4
    Used for data types in learn module

Scikit-learn - https://pypi.python.org/pypi/scikit-learn/0.17b1
    Used for learners inlearn module

db_operations.py
----------------
A short script which was intended to combine the databases generated by each partner
who compiled half of the books into a unified database. Had we more extensively
utilized the database it would have more functions. The one function it does have
has some hardcoded file paths and should probably not be run unless tweaked accordingly.

learn.py
--------
The bulk of the machine learning is in this file. It requires a directory full of
sub-directories labeled by genre which each contain text files representing books in
that genre. The main directory and labels of folders are in the 'book_directory'
and 'all_genres' global variables at the top of the file. Example folder structure:

    C:\\books
        C:\\books\\Adventure
            TomSawyer.txt
        C:\\books\\Science Fiction
            WarOfTheWorlds.txt
    ...

The file includes functions to extract the data from the books in the above structure,
split the data into training and test data, extract text features, perform fitting and
prediction (Linear Regression, Multinomial Naieve Bayes),ave intermediary results to
a pickle file, generate classification reports and confusion matrices, and various other
functions.

process_gutenberg.py
--------------------
There are four separate routines consisting of several functions within this file,
designed to be run independently. They could be included in separate modules but we condensed
them to reduce module bloat since this pipeline was abandoned due to query rate limitations.

The method for running each of these routines in sequence we took was:
    Download the Project Gutenberg corpus from a mirror to a local directory
    Run the Process Files routine on the corpus to prune files, extract info, and condense
    Run the Scrape Genre Listing routine to get genres to map, update genre mapping functions
    Run the Process Books routine on the condensed directory to grab all book information
    (If errors in process, run Strip Prefixes and delete database to restart if necessary)

Global variables which need changing to run on a new system:
    nameddir - location of collapsed books directory written to by process files routine
    goodreads_cache - cache directory for goodreads html pages scraped
    gbooks_cache - cache directory for google books html pages scraped
    root - root directory of downloaded gutenberg mirror before running process files routine

Process Files Routine
    Checks all of the files in a local version of the Project Gutenberg website directory,
    removes invalid or unusable books, extracts title and author, then rewrites the book file
    into a condensed directory with file name Title%%%Author.txt.

Process Books Routine
    This routine is run via process_all_books, which loops calls to process_raw_book within a
    specified directory containing verified valid books with Title%%%Author.txt file names.
    Each book is run through a pipeline to get all its information and insert it into the
    database: get_isbn -> get_genre_info -> map_genres -> insert_into_db

Scrape Genre Listing Routine
    This function grabs all of the genres listed on Goodreads and puts them in a dictionary
    with their frequency. We then manually selected the top 100 genres listed and mapped them
    into a shortened genre list for use in map_genres in the process_books pipeline.

Strip Prefixes Routine
    Files which were processed and were successful or had verified errors
    had their file names prepended with a string enabling us to skip them in subsequent runs. These
    two functions remove those prefixes from all files in a folder to reset if necessary.


process_manybooks.py
--------------------
There are two separate routines in this file designed to be run independently. The manybooks
pipeline is quicker and more fluid since all of the information we needed could be extracted
from the same place as the downloaded books.

Global variables which need changing to run on a new system:
    dl_dir - base directory to download files into for download books routine
    microsoft_invalid - list containing invalid characters in file name on given OS

Download Books Routine
    Run via download_all_books, this initializes a script which downloads all of the books from the
    genres defined at the top in the 'categories' and 'XXXXX_links' variables from manybooks. Editing
    the three lines marked with to-do's in the download function enables running on a subset of the
    category pages. This routine invokes generate_book_download_urls, download_books, open_url,
    and insert_into_db. Console debug information allows the tracking of progress, with each
    category link being traversed printed out and the encoding of each book printed as it's
    downloaded.

Sanitize Texts Routine
    Short routine to be run on a single directory specified as input, which invokes the gutenberg
    strip_headers function to remove copyright and metadata information from the contained files.

statistics.py
-------------
Contains a few basic functions to get average file length, average character count, line
counts, and generate a line count histogram for all text files within a given directory.